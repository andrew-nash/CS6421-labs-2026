{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb59b144-0160-4fe9-84d0-c85d5f8cb36e",
      "metadata": {
        "id": "cb59b144-0160-4fe9-84d0-c85d5f8cb36e"
      },
      "source": [
        "# Graded Lab 04\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs-2026/blob/main/Lab04.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This lab will guide you through the implementation of a CNN-based autoencoder for image anomaly detection.\n",
        "\n",
        "The main purpose of the lab is to encourage you to develop the practical skill of Hyper-parameter optimization. To do this, we will employ some architectures, namely the CNN and Autoencoder, that you will see much more detail of in upcoming lectures. This lab introduces the concepts of both Autoencoders and CNNs. This lab remains agnostic of the specifics of how CNNs are implemented, we will spare this detail for a future lab.\n",
        "\n",
        "Marks will be awarded for each task.\n",
        "\n",
        "\n",
        "Within each task, 20% of marks (1 mark) will be awarded for meeting the specified performance criteria.\n",
        "\n",
        "The remaining 80% of marks (4 marks) are awarded based on the quality of explanation. The scale of marking is as follows\n",
        "\n",
        "\n",
        "0. No explanation or justification of final chosen model\n",
        "1. Little, or incorrect explanation of the observed results, and justification of the final choice of model\n",
        "2. Some partially correct explanation of the results, and justification of the final choice of model\n",
        "3. Valid explanation of the results, but no justification of the final choice of model (and vice versa)\n",
        "4. Valid explanation of the results, positive justification of the final choice of model\n",
        "\n",
        "Within your explanations, you should consider the train and validation loss, any indications of under- and over- fitting, generalisation, whether vanishing gradients are present,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "494a2dec-2165-4063-ba22-e27e193e2c12",
      "metadata": {
        "id": "494a2dec-2165-4063-ba22-e27e193e2c12",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import requests # Added for downloading the dataset\n",
        "import os       # Added for file path operations\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import datetime\n",
        "\n",
        "# Specify the device on which this will be run\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ee794c-8682-4955-86b7-5fdf740d69f5",
      "metadata": {
        "id": "01ee794c-8682-4955-86b7-5fdf740d69f5"
      },
      "source": [
        "# Data\n",
        "\n",
        "The dataset that we will use is the UCI dataset for recognition of handwritten digits https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits  \n",
        "\n",
        "Each sample consists of an 8 pixel by 8 pixel greyscale image of a digit. Each pixel is represented by an integer in the range [0,16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4de8152f-7427-423b-9890-07c39b21db75",
      "metadata": {
        "id": "4de8152f-7427-423b-9890-07c39b21db75",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class UCI_Digits_Dataset(torch.utils.data.Dataset):\n",
        "    # 8,12,0,16, . . 15,7\n",
        "    # 64 pixel values [0-16], digit [0-9]\n",
        "\n",
        "    def __init__(self, src_file, n_rows=None):\n",
        "        # the data is stored as a flat vector of 65 values - 64 pixels, and a label\n",
        "        all_xy = np.loadtxt(src_file, max_rows=n_rows,\n",
        "          usecols=range(0,65), delimiter=\",\", comments=\"#\",\n",
        "          dtype=np.float32)\n",
        "\n",
        "        # reshape this so each image is stored as an [8x8] matrix\n",
        "\n",
        "        self.img_data = torch.tensor(all_xy[:,0:64], dtype=torch.float).to(device).reshape(-1,8,8)\n",
        "        # normalise so pixel values are in the range [0,1]\n",
        "        self.img_data/=16.0\n",
        "        self.label_data = torch.tensor(all_xy[:,64], dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data[idx],self.label_data[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fd11169-3c8c-4c22-9262-0bf9bb5deb9d",
      "metadata": {
        "id": "0fd11169-3c8c-4c22-9262-0bf9bb5deb9d"
      },
      "source": [
        "We can define a function that allows us to simply extract single data samples from the dataset, and visualise them as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6a2372eb-fab0-4d19-b131-edee28dc54bc",
      "metadata": {
        "id": "6a2372eb-fab0-4d19-b131-edee28dc54bc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def display_digit(ds, idx, save=False):\n",
        "    # ds is a PyTorch Dataset\n",
        "    X, label = ds[idx]  # tensor\n",
        "    img = np.array(X)  # numpy row of pixels\n",
        "    print(\"\\ndigit = \", str(label), \"\\n\")\n",
        "\n",
        "    for i in range(8):\n",
        "        for j in range(8):\n",
        "            pxl = img[i,j]  # or [i][j] syntax\n",
        "            pxl = int(pxl * 16.0)  # denormalize\n",
        "            print(\"%.2X\" % pxl, end=\"\")\n",
        "            print(\" \", end=\"\")\n",
        "        print(\"\")\n",
        "\n",
        "    plt.imshow(img, cmap=plt.get_cmap('gray_r'))\n",
        "    if save == True:\n",
        "        plt.savefig(\"./idx_\" + str(idx) + \"_digit_\" +\n",
        "        str(label) + \".jpg\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def display_digits(ds, idxs, save=False):\n",
        "    # idxs is a list of indices\n",
        "    for idx in idxs:\n",
        "        display_digit(ds, idx, save)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313bb2b2-8cfc-4c86-8435-138d293dfdd7",
      "metadata": {
        "id": "313bb2b2-8cfc-4c86-8435-138d293dfdd7"
      },
      "source": [
        "## Downloading the Data and Creating the torch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f1285f65-35e0-4837-9ab8-8e3aadfa8a2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1285f65-35e0-4837-9ab8-8e3aadfa8a2f",
        "outputId": "0a1c4eee-8460-4bdf-b867-efc2a088a6f9",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Begin UCI Digits autoencoder anomaly demo \n",
            "\n",
            "Loading data as normalized tensors \n",
            "Downloading dataset from https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra to optdigits.tra\n",
            "Download complete.\n"
          ]
        }
      ],
      "source": [
        "# 0. get started\n",
        "print(\"\\nBegin UCI Digits autoencoder anomaly demo \")\n",
        "\n",
        "# 1. create Dataset object\n",
        "print(\"\\nLoading data as normalized tensors \")\n",
        "\n",
        "# Download the dataset from URL\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\"\n",
        "local_file_name = \"optdigits.tra\"\n",
        "\n",
        "print(f\"Downloading dataset from {data_url} to {local_file_name}\")\n",
        "try:\n",
        "    response = requests.get(data_url)\n",
        "    response.raise_for_status() # Raise an exception for HTTP errors\n",
        "    with open(local_file_name, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Download complete.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the dataset: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3d241b25-18ea-4c9c-bfbf-d6f6108cfc58",
      "metadata": {
        "id": "3d241b25-18ea-4c9c-bfbf-d6f6108cfc58",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_ds = UCI_Digits_Dataset(local_file_name)  # all rows\n",
        "\n",
        "train, val = torch.utils.data.random_split(data_ds, [0.8,0.2], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6efd4d46-08e7-4c44-bef4-a84fb4319b37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "6efd4d46-08e7-4c44-bef4-a84fb4319b37",
        "outputId": "534c9429-0b1b-4332-a02b-dfab415f5f9f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "digit =  tensor(9.) \n",
            "\n",
            "00 01 10 10 0F 05 00 00 \n",
            "00 01 10 0D 09 0F 03 00 \n",
            "00 03 10 00 05 10 05 00 \n",
            "00 00 0C 10 10 10 06 00 \n",
            "00 00 01 04 04 0C 05 00 \n",
            "00 00 00 00 00 0C 09 00 \n",
            "00 00 01 00 02 0E 07 00 \n",
            "00 00 0C 10 10 0F 03 00 \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_digit(train, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbe4ee2-f3e5-4dd7-89f4-d211e3d8f6f4",
      "metadata": {
        "id": "8cbe4ee2-f3e5-4dd7-89f4-d211e3d8f6f4"
      },
      "source": [
        "# Introduction to Auto-encoders\n",
        "\n",
        "In this assignment, we will create a **simple autoencoder** model.\n",
        "\n",
        "\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n",
        "\n",
        " 1) Autoencoders are _data-specific_, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n",
        "\n",
        "2) Autoencoders are _lossy_, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n",
        "\n",
        "3) Autoencoders are _learned automatically from data examples_, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n",
        "\n",
        "To build an autoencoder, you need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a \"loss\" function). The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent.\n",
        "\n",
        "In general, a neural network is a computational model that is used for finding a function describing the relationship between data features $x$ and its values or labels $y$, i.e. $y = f(x)$.\n",
        "An autoencoder is specific type of neural network, which consists of encoder and decoder components: (1) the **encoder**, which learns a compressed data representation $z$, and (2) the **decoder**, which reconstructs the data $\\hat{x}$ based on its idea $z$ of how it is structured:\n",
        "$$ z = f\\big(h_{e}(x)\\big)$$\n",
        "$$ \\hat{x} = f\\big(h_{d}(z)\\big),$$\n",
        "where $z$ is the learned data representation by encoder $h_{e}$, and $\\hat{x}$ is the reconstructed data by decoder $h_{d}$ based on $z$.\n",
        "\n",
        "\n",
        "The encoder and decoder are defined as:\n",
        "$$ z = f\\big(h_{e}(x)\\big)$$\n",
        "$$ \\hat{x} = f\\big(h_{d}(z)\\big),$$\n",
        "where $z$ is the compressed data representation generated by encoder $h_{e}$, and $\\hat{x}$ is the reconstructed data generated by decoder $h_{d}$ based on $z$.\n",
        "\n",
        "<div align=\"center\"><img src=\"https://github.com/benjaminirving/mlseminars-autoencoders/blob/master/imgs/d1.png?raw=1\" width=\"80%\"></div>\n",
        "\n",
        "In this figure, we take as input an image, and compress that image before decompressing it using a neural network.\n",
        "\n",
        "There are many resources that explain the operation of Autoencoders online. Particularly interesing is https://compneuro.neuromatch.io/tutorials/Bonus_Autoencoders/student/Bonus_Tutorial1.html, which provides an excellent example of an Autoencoder for a similar use case, as well as an interesting comparison of deep encoders to PCA decomposition.\n",
        "\n",
        "Some of the key uses of deep autoencoders are:\n",
        "\n",
        "1. Data compression\n",
        "2. Denoising\n",
        "3. Anomaly Detection\n",
        "\n",
        "You will soon see more about Autoencoders in upcoming lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9add374a-5e92-46f2-ad21-005b9aecbb7b",
      "metadata": {
        "id": "9add374a-5e92-46f2-ad21-005b9aecbb7b"
      },
      "source": [
        "## A Basic Autoencoder Using Fully Connected Layers\n",
        "\n",
        "The following is a basic example of an autoencoder on this data, based on the basic fully connected neural network that we have seen thus far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3f6b42ca-7979-4458-855f-98cc2a601761",
      "metadata": {
        "id": "3f6b42ca-7979-4458-855f-98cc2a601761",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def fc_train_loop(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
        "\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for batch, (X,label) in enumerate(dataloader):\n",
        "        # We will pass in 2D images, but want to work with simple vector inputs\n",
        "        # And so we willl 'Flatten' all inputs -- i.e., reshape it from (batch, 8, 8)\n",
        "        # to (batch, 64)\n",
        "        X = X.float().reshape(-1, 64)\n",
        "\n",
        "        # IMPORTANT, WE DON'T USE THE DATASET'S OWN Y\n",
        "        # INSTEAD WE FIT AGAINST X\n",
        "        Y = X\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "\n",
        "        loss = loss_fn(pred, Y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # every 10th batch, log the gradients\n",
        "        if batch % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                for name, param in model.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
        "\n",
        "        # now zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "    ## Log Metrics in TensorBoard\n",
        "    avg_batchloss = total_loss / num_batches\n",
        "    writer.add_scalar('Loss/train', avg_batchloss, epoch)\n",
        "    print(f\"Train Error: Avg loss: {avg_batchloss:>8f} \\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fc_val_loop(dataloader, model, loss_fn, epoch, writer):\n",
        "\n",
        "\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    test_batchloss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, label in dataloader:\n",
        "            X = X.float().reshape(-1, 64)\n",
        "            Y = X\n",
        "\n",
        "            pred = model(X)\n",
        "            test_batchloss += loss_fn(pred, Y).item()\n",
        "\n",
        "    test_batchloss /= num_batches\n",
        "\n",
        "    writer.add_scalar('Loss/val', test_batchloss, epoch)\n",
        "    print(f\"Val Error: Avg loss: {test_batchloss:>8f} \\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "23295cbf-b91c-43a5-a4e9-b9c54636413d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23295cbf-b91c-43a5-a4e9-b9c54636413d",
        "outputId": "83ca13b9-18ac-46d0-c033-e7ac76b38a73",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BasicFCAutoencoder(\n",
            "  (relu): ReLU()\n",
            "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
            "  (fc3): Linear(in_features=8, out_features=32, bias=True)\n",
            "  (fc4): Linear(in_features=32, out_features=64, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class BasicFCAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(BasicFCAutoencoder, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 32)\n",
        "        self.fc2 = nn.Linear(32, 8)\n",
        "        self.fc3 = nn.Linear(8, 32)\n",
        "        self.fc4 = nn.Linear(32, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "# for an autoencoder, input and output sizes\n",
        "# must be identical\n",
        "input_size = output_size = 64\n",
        "\n",
        "model = BasicFCAutoencoder(input_size, output_size)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "88177977-d461-403e-bfdb-cb83a1323e31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88177977-d461-403e-bfdb-cb83a1323e31",
        "outputId": "ef853dfd-72dd-4efd-fe39-6405e9c9142e",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.208512 \n",
            "\n",
            "Val Error: Avg loss: 0.206605 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.206839 \n",
            "\n",
            "Val Error: Avg loss: 0.204947 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.205178 \n",
            "\n",
            "Val Error: Avg loss: 0.203302 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.203529 \n",
            "\n",
            "Val Error: Avg loss: 0.201673 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.201901 \n",
            "\n",
            "Val Error: Avg loss: 0.200072 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 16\n",
        "epochs = 5\n",
        "\n",
        "run_name = f\"logs/BasicModel/{datetime.datetime.now().strftime('%b%d_%H-%M-%S')}\"\n",
        "writer = SummaryWriter(run_name)\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "val_dataloader  =  DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for t in range(1, epochs+1):\n",
        "    print(f\"Epoch {t}\\n-------------------------------\")\n",
        "    fc_train_loop(train_dataloader, model, loss_fn, optimizer, t, writer)\n",
        "    fc_val_loop(val_dataloader, model, loss_fn, t, writer)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7846b7-ad2e-4e8b-9273-348873cb34de",
      "metadata": {
        "id": "9b7846b7-ad2e-4e8b-9273-348873cb34de"
      },
      "source": [
        "# Introduction to CNNs\n",
        "\n",
        "\n",
        "## CNN Layers\n",
        "Convolutional Neural Networks are also something you will see more details of in upcoming lectures.\n",
        "\n",
        "Consider how the data which we are passing to our model consists of 8x8 images, which we convert into 64-length vectors. These vectors discrad the spatial relationships within the grid of pixels that make up the image.\n",
        "\n",
        "Convolutional Neural Nets provide a basic architecture that preserves inputs as 2D images. Instead of maintaining a *weight matrix*, it maintaines a set of (usually square) *filters* (also known as *kernels*), which are smaller than the size of the input image.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://media.datacamp.com/legacy/v1700043905/image10_f8b261ebf1.png' width='750px'>\n",
        "\n",
        "Each $k\\times k$ filter is applied to $k\\times k$ sub-regions of the inputted image, with a convolution operation.\n",
        "<table>\n",
        "    <tr>\n",
        "        <td><img src='https://miro.medium.com/max/2340/1*Fw-ehcNBR9byHtho-Rxbtw.gif' width=\"500px\"/></td><td><img src='https://towardsdatascience.com/wp-content/uploads/2020/08/1r13ZUdVTQwVuhDPmo3JKag.png' width='500px'/></td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "Typically, each CNN layer will contain a number of equally sized filters operating in parallel.\n",
        "\n",
        "## Pooling Layers\n",
        "\n",
        "Following each CNN layer, the dimension of the resulting activation map is usually reduced by means of a Pooling layer. This applies a $p\\times p$ grid over non-overlapping regions of the activation map. Only the maximum value in each grid is retained in the output\n",
        "\n",
        "<img src='https://towardsdatascience.com/wp-content/uploads/2020/08/1sK7oP1m129V_oNGSsHIm_w.png' width='500px'/>\n",
        "\n",
        "\n",
        "You see the detailed mathematics of this in the lecture material.\n",
        "\n",
        "## Deconvolution/Transposed Convolution\n",
        "\n",
        "In order to take the compressed intermediate represenation, and reproduce an image of the same dimension as the input, we require some inverse function for the pooling operation - i.e., taking a small image/activation map, and producing a larger activation map from it. The function we use for this is the Transpose Convolution. This takes a single pixel from the input map, and multiplies it by a kernel/filter. These maps are then combined to form an enlarged output map.\n",
        "\n",
        "https://medium.com/data-science/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967\n",
        "\n",
        "(All images credit: https://medium.com/sfu-cspmp/an-introduction-to-convolutional-neural-network-cnn-207cdb53db97)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9bf32c-fde4-48f6-b9c2-9cd9685a824c",
      "metadata": {
        "id": "8e9bf32c-fde4-48f6-b9c2-9cd9685a824c"
      },
      "source": [
        "# Defining and Training a CNN Based Autoencoder\n",
        "\n",
        "We will require new train and validation loops, which will pass data to the model as batches of $8\\times 8$ images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4ae77858-2044-4a7c-9bea-1c0658352a08",
      "metadata": {
        "id": "4ae77858-2044-4a7c-9bea-1c0658352a08"
      },
      "outputs": [],
      "source": [
        "def cnn_train_loop(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
        "\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for batch, (X,label) in enumerate(dataloader):\n",
        "\n",
        "\n",
        "        # We will pass in 2D images, without flattening\n",
        "        # However, we still need to reshape ...\n",
        "        # This is to add channel information to the data, even though\n",
        "        # it is a single channel image\n",
        "        X = X.float().reshape(-1,1,8,8)\n",
        "        # IMPORTANT, WE DON'T USE THE DATASET'S OWN Y\n",
        "        # INSTEAD WE FIT AGAINST X\n",
        "        Y = X\n",
        "\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, Y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # every 10th batch, log the gradients\n",
        "        if batch % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                for name, param in model.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
        "\n",
        "        # now zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "    ## Log Metrics in TensorBoard\n",
        "    avg_batchloss = total_loss / num_batches\n",
        "    writer.add_scalar('Loss/train', avg_batchloss, epoch)\n",
        "    print(f\"Train Error: Avg loss: {avg_batchloss:>8f} \\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cnn_val_loop(dataloader, model, loss_fn, epoch, writer):\n",
        "\n",
        "\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    test_batchloss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, label in dataloader:\n",
        "            # We will pass in 2D images, without flattening\n",
        "            # However, we still need to reshape ...\n",
        "            # This is to add channel information to the data, even though\n",
        "            # it is a single channel image\n",
        "            X = X.float().reshape(-1,1,8,8)\n",
        "            # IMPORTANT, WE DON'T USE THE DATASET'S OWN Y\n",
        "            # INSTEAD WE FIT AGAINST X\n",
        "            Y = X\n",
        "\n",
        "            pred = model(X)\n",
        "            test_batchloss += loss_fn(pred, Y).item()\n",
        "\n",
        "    test_batchloss /= num_batches\n",
        "\n",
        "    writer.add_scalar('Loss/val', test_batchloss, epoch)\n",
        "    print(f\"Val Error: Avg loss: {test_batchloss:>8f} \\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f961ee1b-64bf-49ca-9e8a-52bd93fad9fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f961ee1b-64bf-49ca-9e8a-52bd93fad9fd",
        "outputId": "3ee73ae6-d9fb-4826-b2d2-fa55149e06ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BasicCNNAutoencoder(\n",
            "  (relu): ReLU()\n",
            "  (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (deconv1): ConvTranspose2d(4, 1, kernel_size=(4, 4), stride=(2, 2))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class BasicCNNAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BasicCNNAutoencoder, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "        number_filters = 4\n",
        "        filter_size = 2\n",
        "        pool_size = 2\n",
        "\n",
        "        ### ENCODER\n",
        "\n",
        "        # in_channels is the number of channels in the input image. The fact that our inputs\n",
        "        # are matrices (shape 8x8) of floats means that they are single-channel images\n",
        "        # Images with separate Red, Green and Blue channel inputs (shape 8x8x3) would require 3 in_channels\n",
        "\n",
        "        # out_channels is the number of kernels/filters to use\n",
        "\n",
        "        # kernel_size is the width and height of the filters to apply\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=number_filters, kernel_size=filter_size)\n",
        "        # the size of pooling to apply\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=pool_size)\n",
        "\n",
        "        # In practice, you can apply more convolution layers here ...\n",
        "        # at this stage, you need to check the number of activations after pool1\n",
        "\n",
        "\n",
        "        ### DECODER\n",
        "\n",
        "        # the stride MUST be set equal to the pool_size of the max_pool, otherwise the resuling output will not have the correct shape\n",
        "        self.deconv1 = nn.ConvTranspose2d(in_channels=number_filters, out_channels=1, kernel_size=number_filters, stride=pool_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.deconv1(x)\n",
        "        return x\n",
        "\n",
        "# for an autoencoder, input and output sizes\n",
        "# must be identical\n",
        "input_size = output_size = 64\n",
        "\n",
        "model = BasicCNNAutoencoder()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "102e91ae-528f-4b55-b2cb-cbdd76372aa6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "102e91ae-528f-4b55-b2cb-cbdd76372aa6",
        "outputId": "0f454276-6449-4660-cbaf-c73b546abfa4",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.142032 \n",
            "\n",
            "Val Error: Avg loss: 0.140982 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.140881 \n",
            "\n",
            "Val Error: Avg loss: 0.140110 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.140125 \n",
            "\n",
            "Val Error: Avg loss: 0.139485 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.139547 \n",
            "\n",
            "Val Error: Avg loss: 0.138970 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.139051 \n",
            "\n",
            "Val Error: Avg loss: 0.138502 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 16\n",
        "epochs = 5\n",
        "\n",
        "run_name = f\"logs/BasicCNNModel/{datetime.datetime.now().strftime('%b%d_%H-%M-%S')}\"\n",
        "writer = SummaryWriter(run_name)\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "val_dataloader  =  DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for t in range(1, epochs+1):\n",
        "    print(f\"Epoch {t}\\n-------------------------------\")\n",
        "    cnn_train_loop(train_dataloader, model, loss_fn, optimizer, t, writer)\n",
        "    cnn_val_loop(val_dataloader, model, loss_fn, t, writer)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e714b0b7-999d-4f6f-bc3a-2fe28df4c6e5",
      "metadata": {
        "id": "e714b0b7-999d-4f6f-bc3a-2fe28df4c6e5"
      },
      "source": [
        "### Determining the bottleneck/intermediate representation size\n",
        "\n",
        "It is not so easy for this particular model (compared to the feedforward model), to determine what the dimension of the encoded representation/bottleneck/latent/intermediate representation is.\n",
        "\n",
        "The torchinfo module can give us better insights into our model to make this, and other observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4264553a-0759-47b1-a56a-69d2a435de46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4264553a-0759-47b1-a56a-69d2a435de46",
        "outputId": "c0ffca25-8476-4c02-dccd-844b9bd83100",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "%pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "089826e2-bf2c-4cc6-95b1-29110b9c42b8",
      "metadata": {
        "id": "089826e2-bf2c-4cc6-95b1-29110b9c42b8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6e50a6ce-a52e-4190-b102-651bcddbbdbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e50a6ce-a52e-4190-b102-651bcddbbdbc",
        "outputId": "0dfc0ab7-22d1-42cf-8d21-054b2e41ca42",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "BasicCNNAutoencoder                      [1, 10, 10]               --\n",
              "├─Conv2d: 1-1                            [4, 8, 8]                 20\n",
              "├─ReLU: 1-2                              [4, 8, 8]                 --\n",
              "├─MaxPool2d: 1-3                         [4, 4, 4]                 --\n",
              "├─ConvTranspose2d: 1-4                   [1, 10, 10]               65\n",
              "==========================================================================================\n",
              "Total params: 85\n",
              "Trainable params: 85\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model, input_data = torch.zeros(1,8,8, device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad820c3c-18ae-4119-823b-e968dd374811",
      "metadata": {
        "id": "ad820c3c-18ae-4119-823b-e968dd374811"
      },
      "source": [
        "We can see that this model's bottleneck size is $4*3*3=36$. We can also see that the model has only 85 total trainable parameters (weights), significantly smaller than the feedforward model, which has equivalent performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f270932a-1724-441c-855e-407964973d23",
      "metadata": {
        "id": "f270932a-1724-441c-855e-407964973d23"
      },
      "source": [
        "## Multi-layer Encoder and Decoder\n",
        "\n",
        "The previous CNN model featured only one and CNN and pooling layer in the Encoder, and one deconvolution. Here, we demonstrate a multi-layer CNN autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fbcd2d7c-0c0a-418b-a1ae-51a360940d99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbcd2d7c-0c0a-418b-a1ae-51a360940d99",
        "outputId": "e8ed0033-a11a-40c1-eea1-55f44cfbd0eb",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiCNNAutoencoder(\n",
            "  (relu): ReLU()\n",
            "  (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(1, 1), padding=same)\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(4, 2, kernel_size=(2, 2), stride=(1, 1), padding=same)\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (deconv1): ConvTranspose2d(2, 2, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (deconv2): ConvTranspose2d(2, 1, kernel_size=(2, 2), stride=(2, 2))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class MultiCNNAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiCNNAutoencoder, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        ### ENCODER\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, padding='same')\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=2, padding='same')\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "\n",
        "        ### DECODER\n",
        "        # we should have 2 decovolution layers, as we had 2 pooling layers. An alternative would be to have\n",
        "        # a single decovolution layer with both kernel size and stride 4\n",
        "        self.deconv1 = nn.ConvTranspose2d(in_channels=2, out_channels=2, kernel_size=2, stride=2)\n",
        "        self.deconv2 = nn.ConvTranspose2d(in_channels=2, out_channels=1, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.deconv2(x)\n",
        "        return x\n",
        "\n",
        "# for an autoencoder, input and output sizes\n",
        "# must be identical\n",
        "input_size = output_size = 64\n",
        "\n",
        "model = MultiCNNAutoencoder()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "37109ef4-0cb4-44a7-b53d-c0b5d232ee40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37109ef4-0cb4-44a7-b53d-c0b5d232ee40",
        "outputId": "2bd32c4b-7898-44c6-97ec-59704d784db1",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MultiCNNAutoencoder                      [1, 8, 8]                 --\n",
              "├─Conv2d: 1-1                            [4, 8, 8]                 20\n",
              "├─ReLU: 1-2                              [4, 8, 8]                 --\n",
              "├─MaxPool2d: 1-3                         [4, 4, 4]                 --\n",
              "├─Conv2d: 1-4                            [2, 4, 4]                 34\n",
              "├─ReLU: 1-5                              [2, 4, 4]                 --\n",
              "├─MaxPool2d: 1-6                         [2, 2, 2]                 --\n",
              "├─ConvTranspose2d: 1-7                   [2, 4, 4]                 18\n",
              "├─ReLU: 1-8                              [2, 4, 4]                 --\n",
              "├─ConvTranspose2d: 1-9                   [1, 8, 8]                 9\n",
              "==========================================================================================\n",
              "Total params: 81\n",
              "Trainable params: 81\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model, input_data = torch.zeros(1,8,8, device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e50542-e216-46f3-ad76-438cb1e9ee37",
      "metadata": {
        "id": "31e50542-e216-46f3-ad76-438cb1e9ee37"
      },
      "source": [
        "Notice here that the intermediate representation has size $2*2*2=8$ weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ab512437-57f0-4880-8f90-8c14177384e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab512437-57f0-4880-8f90-8c14177384e3",
        "outputId": "077491e5-6d50-4406-e379-b4ad351f6a9f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.425869 \n",
            "\n",
            "Val Error: Avg loss: 0.328450 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.273793 \n",
            "\n",
            "Val Error: Avg loss: 0.228128 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.203302 \n",
            "\n",
            "Val Error: Avg loss: 0.181735 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.170628 \n",
            "\n",
            "Val Error: Avg loss: 0.160306 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train Error: Avg loss: 0.155482 \n",
            "\n",
            "Val Error: Avg loss: 0.150423 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 16\n",
        "epochs = 5\n",
        "\n",
        "run_name = f\"logs/MultiCNNModel/{datetime.datetime.now().strftime('%b%d_%H-%M-%S')}\"\n",
        "writer = SummaryWriter(run_name)\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "val_dataloader  =  DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for t in range(1, epochs+1):\n",
        "    print(f\"Epoch {t}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, t, writer)\n",
        "    val_loop(val_dataloader, model, loss_fn, t, writer)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54468a6-5c81-4b84-8bfe-e0ebb2d929ed",
      "metadata": {
        "id": "f54468a6-5c81-4b84-8bfe-e0ebb2d929ed"
      },
      "source": [
        "# Hyper-parameter Searching, Graded Task\n",
        "\n",
        "The graded aspect of this lab will be to evaluate the peformance of different CNN based autoencoders for various choices of hyper-parameter. The following is an example of searhcing over numbers of layers, and layer sizes for the simple Fully Connected Autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81920cb1-3d28-4817-8528-40c52fdf6f39",
      "metadata": {
        "id": "81920cb1-3d28-4817-8528-40c52fdf6f39",
        "outputId": "88210d68-a7ca-4af0-8f1c-d599b1a3e48c",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "HSFCAutoencoder                          [64]                      --\n",
              "├─ModuleList: 1-1                        --                        --\n",
              "│    └─Linear: 2-1                       [32]                      2,080\n",
              "│    └─ReLU: 2-2                         [32]                      --\n",
              "│    └─Linear: 2-3                       [16]                      528\n",
              "│    └─ReLU: 2-4                         [16]                      --\n",
              "│    └─Linear: 2-5                       [32]                      544\n",
              "│    └─ReLU: 2-6                         [32]                      --\n",
              "│    └─Linear: 2-7                       [64]                      2,112\n",
              "│    └─ReLU: 2-8                         [64]                      --\n",
              "==========================================================================================\n",
              "Total params: 5,264\n",
              "Trainable params: 5,264\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.23\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.02\n",
              "Estimated Total Size (MB): 0.02\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class HSFCAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_layers, layer_sizes):\n",
        "        super(HSFCAutoencoder, self).__init__()\n",
        "\n",
        "        # this is a way of storing a variable number of layers, rather than using self.xxxx\n",
        "        self.layers = nn.ModuleList()\n",
        "        layer_input_size = input_size\n",
        "\n",
        "        for layer_size in layer_sizes:\n",
        "            self.layers.append(nn.Linear(layer_input_size, layer_size))\n",
        "            # the next layer's input size is this layer's output size\n",
        "            layer_input_size=layer_size\n",
        "            # we could apply the activations here, or define a single relu() above, and apply it\n",
        "            # in forward()\n",
        "            self.layers.append(nn.ReLU())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "        return x\n",
        "\n",
        "# for an autoencoder, input and output sizes\n",
        "# must be identical\n",
        "input_size = output_size = 64\n",
        "\n",
        "eg_model = HSFCAutoencoder(input_size, output_size, 4, [32,16,32,64])\n",
        "summary(eg_model, input_data = torch.zeros(64, device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1420f53-f2c4-4056-838b-0433ef5617c4",
      "metadata": {
        "id": "d1420f53-f2c4-4056-838b-0433ef5617c4"
      },
      "source": [
        "It is worth re-defining the train and val loops to only print the final losses. Detailed losses can be found in the TensorBoard output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898a9b27-e127-4743-8fff-ab28040f5ce7",
      "metadata": {
        "id": "898a9b27-e127-4743-8fff-ab28040f5ce7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def fc_train_loop(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
        "\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for batch, (X,label) in enumerate(dataloader):\n",
        "        # We will pass in 2D images, but want to work with simple vector inputs\n",
        "        # And so we willl 'Flatten' all inputs -- i.e., reshape it from (batch, 8, 8)\n",
        "        # to (batch, 64)\n",
        "        X = X.float().reshape(-1, 64)\n",
        "\n",
        "        # IMPORTANT, WE DON'T USE THE DATASET'S OWN Y\n",
        "        # INSTEAD WE FIT AGAINST X\n",
        "        Y = X\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "\n",
        "        loss = loss_fn(pred, Y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # every 10th batch, log the gradients\n",
        "        if batch % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                for name, param in model.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
        "\n",
        "        # now zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "    ## Log Metrics in TensorBoard\n",
        "    avg_batchloss = total_loss / num_batches\n",
        "    writer.add_scalar('Loss/train', avg_batchloss, epoch)\n",
        "    #print(f\"Train Error: Avg loss: {avg_batchloss:>8f} \\n\")\n",
        "    return avg_batchloss\n",
        "\n",
        "\n",
        "\n",
        "def fc_val_loop(dataloader, model, loss_fn, epoch, writer):\n",
        "\n",
        "\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    test_batchloss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, label in dataloader:\n",
        "            X = X.float().reshape(-1, 64)\n",
        "            Y = X\n",
        "\n",
        "            pred = model(X)\n",
        "            test_batchloss += loss_fn(pred, Y).item()\n",
        "\n",
        "    test_batchloss /= num_batches\n",
        "\n",
        "    writer.add_scalar('Loss/val', test_batchloss, epoch)\n",
        "    #print(f\"Val Error: Avg loss: {test_batchloss:>8f} \\n\")\n",
        "    return test_batchloss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4de324b-1bb5-4028-8814-9d44725a5649",
      "metadata": {
        "id": "b4de324b-1bb5-4028-8814-9d44725a5649"
      },
      "source": [
        "Searching over a set of possible models:\n",
        "\n",
        "## 4 Layer Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c7b674-bc0a-46db-a90e-ded5a3ef7db7",
      "metadata": {
        "id": "a2c7b674-bc0a-46db-a90e-ded5a3ef7db7",
        "outputId": "6c7ce3a0-aa92-41b7-9b26-55c64b93ca43",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Search [52, 32, 52, 64] ####\n",
            "Final Train Avg loss: 29.019899\n",
            "Final Val Avg loss: 28.990100 \n",
            "\n",
            "Done!\n",
            "\n",
            "### Search [32, 16, 32, 64] ####\n",
            "Final Train Avg loss: 16.448366\n",
            "Final Val Avg loss: 16.067494 \n",
            "\n",
            "Done!\n",
            "\n",
            "### Search [16, 8, 16, 64] ####\n",
            "Final Train Avg loss: 26.577684\n",
            "Final Val Avg loss: 25.066806 \n",
            "\n",
            "Done!\n",
            "\n",
            "### Search [8, 4, 8, 64] ####\n",
            "Final Train Avg loss: 37.516251\n",
            "Final Val Avg loss: 37.322013 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "n_layers = 3\n",
        "layer_size_searches =[\n",
        "    [52,32,52,64],\n",
        "    [32,16,32,64],\n",
        "    [16,8,16,64],\n",
        "    [8,4,8,64],\n",
        "]\n",
        "\n",
        "for search_no, layer_sizes in enumerate(layer_size_searches):\n",
        "    input_size = output_size = 64\n",
        "\n",
        "    print(f'\\n### Search {layer_sizes} ####')\n",
        "\n",
        "    #create the model based on the specific search\n",
        "    model = HSFCAutoencoder(input_size, output_size, 3, layer_sizes)\n",
        "    learning_rate = 1e-3\n",
        "    batch_size = 16\n",
        "    epochs = 10\n",
        "\n",
        "    run_name = f\"logs/FC_4_layers_search_{search_no}/{datetime.datetime.now().strftime('%b%d_%H-%M-%S')}\"\n",
        "    writer = SummaryWriter(run_name)\n",
        "\n",
        "    train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "    val_dataloader  =  DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    for t in range(1, epochs+1):\n",
        "        train_loss = fc_train_loop(train_dataloader, model, loss_fn, optimizer, t, writer)\n",
        "        val_loss = fc_val_loop(val_dataloader, model, loss_fn, t, writer)\n",
        "    print(f\"Final Train Avg loss: {train_loss:>8f}\")\n",
        "    print(f\"Final Val Avg loss: {val_loss:>8f} \\n\")\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0faaa04b-e634-41db-b949-2c6cd22db86b",
      "metadata": {
        "id": "0faaa04b-e634-41db-b949-2c6cd22db86b"
      },
      "source": [
        "## Task 1: Number of Layers, and CNN Layer Parameters\n",
        "\n",
        "This task requires you to perform hyper-parameter tuning over the following sets of hyper-parameters:\n",
        "\n",
        "\n",
        "1. For a simple 1 layer encoder/1 layer decoder model (CONV->POOL->RELU->DECONV): Consider a possible number of channels between 1 and 8 the CONV and DECONV layers. Maintain kernel_size as 2 in all cases\n",
        "2. For a simple 2 layer encoder/2 layer decoder model (CONV->POOL->RELU->CONV->POOL->RELU->DECONV->RELU->DECONV): Consider a possible number of channels between 1 and 8 for each CONV and DECONV layer. Maintain kernel_size as 2 in all cases\n",
        "\n",
        "Number of epochs, batch size and learning rate are at your discretion.\n",
        "\n",
        "Evaluate each, and determine which has, in your view, the best performance as an autoencoder. Refer to both the Training and Validation loss, as well as the size of the bottleneck of particular models of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372a3d0b-aa89-423c-9ded-6d1b46dd20f4",
      "metadata": {
        "id": "372a3d0b-aa89-423c-9ded-6d1b46dd20f4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def hs_train_loop(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
        "\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for batch, (X,label) in enumerate(dataloader):\n",
        "        # We will pass in 2D images, but want to work with simple vector inputs\n",
        "        # And so we willl 'Flatten' all inputs -- i.e., reshape it from (batch, 8, 8)\n",
        "        # to (batch, 64)\n",
        "        X = X.float().reshape(-1, 1, 8, 8)\n",
        "\n",
        "        # IMPORTANT, WE DON'T USE THE DATASET'S OWN Y\n",
        "        # INSTEAD WE FIT AGAINST X\n",
        "        Y = X\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "\n",
        "        loss = loss_fn(pred, Y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # every 10th batch, log the gradients\n",
        "        if batch % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                for name, param in model.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
        "\n",
        "        # now zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "    ## Log Metrics in TensorBoard\n",
        "    avg_batchloss = total_loss / num_batches\n",
        "    writer.add_scalar('Loss/train', avg_batchloss, epoch)\n",
        "    #print(f\"Train Error: Avg loss: {avg_batchloss:>8f} \\n\")\n",
        "    return avg_batchloss\n",
        "\n",
        "\n",
        "\n",
        "def hs_val_loop(dataloader, model, loss_fn, epoch, writer):\n",
        "\n",
        "\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    test_batchloss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, label in dataloader:\n",
        "            X = X.float().reshape(-1, 1, 8, 8)\n",
        "            Y = X\n",
        "\n",
        "            pred = model(X)\n",
        "            test_batchloss += loss_fn(pred, Y).item()\n",
        "\n",
        "    test_batchloss /= num_batches\n",
        "\n",
        "    writer.add_scalar('Loss/val', test_batchloss, epoch)\n",
        "    #print(f\"Val Error: Avg loss: {test_batchloss:>8f} \\n\")\n",
        "    return test_batchloss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb21dae4-13e5-4245-a243-075892a8ee2d",
      "metadata": {
        "id": "eb21dae4-13e5-4245-a243-075892a8ee2d"
      },
      "outputs": [],
      "source": [
        "class HS_1_Layer_CNNAutoencoder(nn.Module):\n",
        "    def __init__(self, .......):\n",
        "        super(HS_1_Layer_CNNAutoencoder, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        ### ENCODER\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels= ...., kernel_size=2, padding='same')\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "\n",
        "\n",
        "        ### DECODER\n",
        "        # we should have 2 decovolution layers, as we had 2 pooling layers. An alternative would be to have\n",
        "        # a single decovolution layer with both kernel size and stride 4\n",
        "        self.deconv1 = nn.ConvTranspose2d(in_channels=2, out_channels=....., kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.deconv1(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d084e8-b217-4c78-a967-1c12c8746ebc",
      "metadata": {
        "id": "75d084e8-b217-4c78-a967-1c12c8746ebc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Search Parameters\n",
        "'''\n",
        "\n",
        "for .... in ....:\n",
        "    input_size = output_size = 64\n",
        "\n",
        "\n",
        "    #create the model based on the specific search\n",
        "    model = HS_1_Layer_CNNAutoencoder(.....)\n",
        "\n",
        "    learning_rate = ....\n",
        "    batch_size = ....\n",
        "    epochs =  ....\n",
        "\n",
        "    run_name = f\"logs/.........../{datetime.datetime.now().strftime('%b%d_%H-%M-%S')}\"\n",
        "    writer = SummaryWriter(run_name)\n",
        "\n",
        "    train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "    val_dataloader  =  DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    for t in range(1, epochs+1):\n",
        "        train_loss = hs_train_loop(train_dataloader, model, loss_fn, optimizer, t, writer)\n",
        "        val_loss = hs_val_loop(val_dataloader, model, loss_fn, t, writer)\n",
        "    print(f\"Final Train Avg loss: {train_loss:>8f}\")\n",
        "    print(f\"Final Val Avg loss: {val_loss:>8f} \\n\")\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8722d1d7-b92b-4ef7-a02d-2363fd664f0d",
      "metadata": {
        "id": "8722d1d7-b92b-4ef7-a02d-2363fd664f0d"
      },
      "outputs": [],
      "source": [
        ".... repeat for HS_2_Layer_CNNAutoencoder\n",
        "\n",
        "class HS_1_Layer_CNNAutoencoder....."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b83ff6d4-1afd-49a8-9fe4-93b95089fe67",
      "metadata": {
        "id": "b83ff6d4-1afd-49a8-9fe4-93b95089fe67",
        "tags": []
      },
      "outputs": [],
      "source": [
        "explanation_of_chosen_model = '''\n",
        "     In this string, explain which out of the models you evaluated, across both 1 and 2 layer CNN autoencoders performed best.\n",
        "\n",
        "     Refer to bottleneck size, and the validaiton and train losses. Explain *why* you think that this model may perform best out of all the alternatives.\n",
        "\n",
        "     In basic terms, explain your choice of number of epochs, batch size and learning rate\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8825c6ad-af13-4497-bcdc-90205745bca6",
      "metadata": {
        "id": "8825c6ad-af13-4497-bcdc-90205745bca6"
      },
      "source": [
        "For the purposes of assessing your work, export this trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417e9ea0-a6c9-4562-b057-8c03ca321da7",
      "metadata": {
        "id": "417e9ea0-a6c9-4562-b057-8c03ca321da7",
        "outputId": "e6f45dea-fb27-4d29-f21f-ac0ad67f9865",
        "tags": []
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'CHOSEN_MODEL_CLASS' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m sample_input = torch.randn(\u001b[32m1\u001b[39m,\u001b[32m8\u001b[39m,\u001b[32m8\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Export the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m exported_program = torch.export.export( \u001b[43mCHOSEN_MODEL_CLASS\u001b[49m(), sample_input)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Save the exported program\u001b[39;00m\n\u001b[32m      8\u001b[39m torch.export.save(exported_program, \u001b[33m'\u001b[39m\u001b[33mTASK1_model.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'CHOSEN_MODEL_CLASS' is not defined"
          ]
        }
      ],
      "source": [
        "# Create a sample input\n",
        "sample_input = torch.randn(1,8,8)\n",
        "\n",
        "# Export the model\n",
        "exported_program = torch.export.export( CHOSEN_MODEL_CLASS(), sample_input)\n",
        "\n",
        "# Save the exported program\n",
        "torch.export.save(exported_program, 'TASK1_model.pt')\n",
        "\n",
        "# Save the trained weights\n",
        "torch.save(CHOSEN_MODEL.state_dict(), 'TASK1_weights.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6facf945-742e-4b0a-944f-73e38c102142",
      "metadata": {
        "id": "6facf945-742e-4b0a-944f-73e38c102142"
      },
      "source": [
        "## Task 2: Dropout\n",
        "\n",
        "Dropout, as discussed in the lecutres, can be used to add regularisation and improve performance. It is applied in PyTorch as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb91a360-1022-4b3b-b487-dd4a4b8ce0e4",
      "metadata": {
        "id": "eb91a360-1022-4b3b-b487-dd4a4b8ce0e4",
        "outputId": "0549fcaf-4f2a-44ed-915e-5ecfe1e584a3",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "HSFCAutoencoder                          [64]                      --\n",
              "├─ModuleList: 1-1                        --                        --\n",
              "│    └─Linear: 2-1                       [32]                      2,080\n",
              "│    └─Dropout: 2-2                      [32]                      --\n",
              "│    └─ReLU: 2-3                         [32]                      --\n",
              "│    └─Linear: 2-4                       [16]                      528\n",
              "│    └─Dropout: 2-5                      [16]                      --\n",
              "│    └─ReLU: 2-6                         [16]                      --\n",
              "│    └─Linear: 2-7                       [32]                      544\n",
              "│    └─Dropout: 2-8                      [32]                      --\n",
              "│    └─ReLU: 2-9                         [32]                      --\n",
              "│    └─Linear: 2-10                      [64]                      2,112\n",
              "│    └─Dropout: 2-11                     [64]                      --\n",
              "│    └─ReLU: 2-12                        [64]                      --\n",
              "==========================================================================================\n",
              "Total params: 5,264\n",
              "Trainable params: 5,264\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.23\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.02\n",
              "Estimated Total Size (MB): 0.02\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 245,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class HSFCAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_layers, layer_sizes):\n",
        "        super(HSFCAutoencoder, self).__init__()\n",
        "\n",
        "        # this is a way of storing a variable number of layers, rather than using self.xxxx\n",
        "        self.layers = nn.ModuleList()\n",
        "        layer_input_size = input_size\n",
        "\n",
        "        for layer_size in layer_sizes:\n",
        "            self.layers.append(nn.Linear(layer_input_size, layer_size))\n",
        "            # HERE IS THE ONLY CHANGE\n",
        "            self.layers.append(nn.Dropout(p=0.2))\n",
        "\n",
        "            # the next layer's input size is this layer's output size\n",
        "            layer_input_size=layer_size\n",
        "            # we could apply the activations here, or define a single relu() above, and apply it\n",
        "            # in forward()\n",
        "            self.layers.append(nn.ReLU())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "        return x\n",
        "\n",
        "# for an autoencoder, input and output sizes\n",
        "# must be identical\n",
        "input_size = output_size = 64\n",
        "\n",
        "eg_model = HSFCAutoencoder(input_size, output_size, 4, [32,16,32,64])\n",
        "summary(eg_model, input_data = torch.zeros(64, device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425e1224-1e87-442c-80ff-233d72d14a5f",
      "metadata": {
        "id": "425e1224-1e87-442c-80ff-233d72d14a5f"
      },
      "source": [
        "### Graded Task:\n",
        "\n",
        "1. Choose a number (2-3) of high perofrming models from the previous question\n",
        "2. Search over a possible range of dropout probabilities on these models.\n",
        "3. Identify the best performing model, and its assoicated dropout probability -- or 0, if dropout is not appropiate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f211def1-ad08-40a8-8d04-e0553f068698",
      "metadata": {
        "id": "f211def1-ad08-40a8-8d04-e0553f068698"
      },
      "outputs": [],
      "source": [
        "explanation_of_chosen_dropout = '''\n",
        "     In this string, explain which out of the models you evaluated, along with what choice of dropout probabilty performed best\n",
        "\n",
        "     Explain *why* you think that this model may perform best out of all the alternatives.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc6cc3a-a671-4f33-9826-dbb15416510c",
      "metadata": {
        "id": "bbc6cc3a-a671-4f33-9826-dbb15416510c"
      },
      "outputs": [],
      "source": [
        "# Create a sample input\n",
        "sample_input = torch.randn(1,8,8)\n",
        "\n",
        "# Export the model\n",
        "exported_program = torch.export.export( CHOSEN_MODEL_CLASS(), sample_input)\n",
        "\n",
        "# Save the exported program\n",
        "torch.export.save(exported_program, 'TASK2_model.pt')\n",
        "\n",
        "# Save the trained weights\n",
        "torch.save(CHOSEN_MODEL.state_dict(), 'TASK2_weights.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7abffb32-1a32-4439-9ed1-de21043889d8",
      "metadata": {
        "id": "7abffb32-1a32-4439-9ed1-de21043889d8"
      },
      "source": [
        "### Task 3: BatchNorm\n",
        "\n",
        "Batch Normalisation, as per the lecture notes can be applied as follows (for CNN models):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9683ca-f44a-4a68-b3ed-21b8f8caa7ef",
      "metadata": {
        "id": "df9683ca-f44a-4a68-b3ed-21b8f8caa7ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BNBasicCNNAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BNBasicCNNAutoencoder, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        number_filters = 4\n",
        "        filter_size = 2\n",
        "        pool_size = 2\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=number_filters, kernel_size=filter_size, padding='same')\n",
        "        # you must pass the number of channels outputted by the previous conv. filter to bacthnorm\n",
        "        self.b_norm1 = nn.BatchNorm2d(number_filters)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=pool_size)\n",
        "        self.deconv1 = nn.ConvTranspose2d(in_channels=number_filters, out_channels=1, kernel_size=number_filters, stride=pool_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.deconv1(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6787d625-e8d9-43d4-b914-caaaa3d472fe",
      "metadata": {
        "id": "6787d625-e8d9-43d4-b914-caaaa3d472fe"
      },
      "source": [
        "### Graded Task:\n",
        "\n",
        "1. Choose the most high perofrming model from Task 1\n",
        "2. Try to add BatchNorm, for some batch sizes between 2 and 64\n",
        "3. Identify whether BatchNorm has improved performance, and for what batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9fd215f-1084-4e17-adda-7316bc9cd36f",
      "metadata": {
        "id": "f9fd215f-1084-4e17-adda-7316bc9cd36f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "explanation_of_chosen_batchnorm = '''\n",
        "     In this string, explain which model you chose, and whether each improved or not with BatchNorm for different batch sizes\n",
        "\n",
        "\n",
        "     Explain *why* this may be\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5e558ed-b31f-40ef-b565-bb6165f65ff5",
      "metadata": {
        "id": "a5e558ed-b31f-40ef-b565-bb6165f65ff5"
      },
      "outputs": [],
      "source": [
        "# Create a sample input\n",
        "sample_input = torch.randn(1,8,8)\n",
        "\n",
        "# Export the model\n",
        "exported_program = torch.export.export( CHOSEN_MODEL_CLASS(), sample_input)\n",
        "\n",
        "# Save the exported program\n",
        "torch.export.save(exported_program, 'TASK3_model.pt')\n",
        "\n",
        "# Save the trained weights\n",
        "torch.save(CHOSEN_MODEL.state_dict(), 'TASK3_weights.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c619b8d-9c01-4bff-ab8b-1c545876256f",
      "metadata": {
        "id": "5c619b8d-9c01-4bff-ab8b-1c545876256f"
      },
      "source": [
        "### Task 4: Regularization\n",
        "As per the lecture, L1 and L2 regularization can be applied to a model by modifying the loss calculation within the fit() function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc92d42c-798f-46e1-808c-6652e49444f1",
      "metadata": {
        "id": "dc92d42c-798f-46e1-808c-6652e49444f1"
      },
      "outputs": [],
      "source": [
        "loss = loss_fn(pred, Y)\n",
        "\n",
        "# For l1 norm\n",
        "lambda_reg = 0.01\n",
        "l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "loss += lambda_reg * l1_norm\n",
        "\n",
        "# or, for l2 norm:\n",
        "l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "loss += lambda_reg * l2_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa58271b-f514-4c50-91b8-a19e3ce0f295",
      "metadata": {
        "id": "fa58271b-f514-4c50-91b8-a19e3ce0f295"
      },
      "source": [
        "### Graded Task:\n",
        "\n",
        "1. Choose the most high perofrming model from Task 1\n",
        "2. Try to add both L1 and L2, for lambda as 0.001, 0.01 and 0.1 in turn\n",
        "3. Identify the effect on performance, and which choice (if you identify L1 and L2 norm as improving perofrmance0 is best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39e9cb03-4d78-47fa-a143-947cf962aaa1",
      "metadata": {
        "id": "39e9cb03-4d78-47fa-a143-947cf962aaa1"
      },
      "outputs": [],
      "source": [
        "explanation_of_chosen_l_norm = '''\n",
        "     In this string, explain which regularisation you chose, and *why*\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09766f81-1f97-4af0-ae1e-5c49bb2f568b",
      "metadata": {
        "id": "09766f81-1f97-4af0-ae1e-5c49bb2f568b"
      },
      "outputs": [],
      "source": [
        "# Create a sample input\n",
        "sample_input = torch.randn(1,8,8)\n",
        "\n",
        "# Export the model\n",
        "exported_program = torch.export.export( CHOSEN_MODEL_CLASS(), sample_input)\n",
        "\n",
        "# Save the exported program\n",
        "torch.export.save(exported_program, 'TASK4_model.pt')\n",
        "\n",
        "# Save the trained weights\n",
        "torch.save(CHOSEN_MODEL.state_dict(), 'TASK4_weights.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c63be2-7f3a-4a45-9ac7-7641676918f2",
      "metadata": {
        "id": "80c63be2-7f3a-4a45-9ac7-7641676918f2"
      },
      "source": [
        "### Task 5: Open Ended Search\n",
        "\n",
        "Combining everything you have learned from tasks 1-4, create a final model that incorporates all of your observations.\n",
        "\n",
        "You may incorporare different activation functions and weight initialisaitons if you desire, but this is not a requirement.\n",
        "\n",
        "Evaluate the performance of this model -- you should discuss, at the minimum\n",
        "\n",
        "1. Train and validation loss\n",
        "2. Whether it shows signs of over or underfitting\n",
        "3. The bottleneck size\n",
        "4. The total number of trainable parameters\n",
        "5. What makes it better and more robust than other models that you considered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c16f3f-8553-4a44-b22b-daf0fae90f21",
      "metadata": {
        "id": "f6c16f3f-8553-4a44-b22b-daf0fae90f21"
      },
      "outputs": [],
      "source": [
        "explanation_of_final_model = '''\n",
        "     In this string, describe the characteristics of your final model. State each hyper-parameter choice you made, and what reason you had for each\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c14a3e2-419b-4496-9aea-309f3cb997a1",
      "metadata": {
        "id": "1c14a3e2-419b-4496-9aea-309f3cb997a1"
      },
      "outputs": [],
      "source": [
        "# Create a sample input\n",
        "sample_input = torch.randn(1,8,8)\n",
        "\n",
        "# Export the model\n",
        "exported_program = torch.export.export( CHOSEN_MODEL_CLASS(), sample_input)\n",
        "\n",
        "# Save the exported program\n",
        "torch.export.save(exported_program, 'TASK5_model.pt')\n",
        "\n",
        "# Save the trained weights\n",
        "torch.save(CHOSEN_MODEL.state_dict(), 'TASK5_weights.pt')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jupyter-student",
      "language": "python",
      "name": "jupyter-eg-kernel-slurm-py-conda-1ja8rhof9"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
