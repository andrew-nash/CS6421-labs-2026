{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graded Lab 02\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs-2026/blob/main/Lab02.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "\n",
        "This lab will be the first graded assessment of the course. It focuses on applying the priciples of modularity, and implementing the mathematics that you have seen in case to build simple neural networks in PyTorch from scratch.\n",
        "\n",
        "Helpful material worth referencing: https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html\n",
        "\n",
        "**IMPORTANT NOTICE** Where you are asked to define a certain module, class or function, do not change the names of the Class or its functions. Your assignement will be auto-graded, and requires these names of these to match the expected names exactly.\n",
        "\n",
        "If you hve questions during the week, you may contact me at a.nash@cs.ucc.ie"
      ],
      "metadata": {
        "id": "YHor1sGqiQgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "uefpDMAIjw6y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data (Already Completed, No Marks)\n",
        "\n",
        "We will re-use the data loading from the previous lab (modified so that it returns te features as Tensors rather than np.arrays). This will not reuqire modification by you, and is not a graded component of the lab. However, you may alter this code if you wish.\n",
        "\n",
        "The task here is to predict the probability of a passenger surviving the Titanic sinking, given a set of features consisting of\n",
        "\n",
        "1. Gender\n",
        "2. Where they embarked\n",
        "3. Their Ticket Class\n",
        "4. Their Age\n",
        "5. The price of their ticket"
      ],
      "metadata": {
        "id": "AJiPbE1liiV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/andrew-nash/CS6421-labs-2026/raw/refs/heads/main/titanic_test.csv\n",
        "!wget https://github.com/andrew-nash/CS6421-labs-2026/raw/refs/heads/main/titanic_train.csv"
      ],
      "metadata": {
        "id": "fD5CZDmnLeuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TitanicDataset (torch.utils.data.Dataset):\n",
        "    # the Train argument defines whether the dataset is being queried for train or test data\n",
        "    # In practice, you would likely be handling separate datasets for each\n",
        "    def __init__(self, file_name, Train=True):\n",
        "        self.dataframe = pd.read_csv(file_name)\n",
        "        #print(self.dataframe.head())\n",
        "        self.dataframe = self.dataframe.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "        self.dataframe = self.dataframe.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "        self.dataframe = self.dataframe.dropna(subset=['Age', 'Embarked', 'Sex', 'Pclass', 'Fare'])\n",
        "\n",
        "        # Instead of using strings (\"Male\" and \"Female\"), we need to convert these to numerical values -- in this\n",
        "        # case 1 for male, 0 for female\n",
        "        self.dataframe['Male'] = np.where(self.dataframe['Sex'] == 'male', 1, 0)\n",
        "\n",
        "        # Manual one-hot encoding for Embarked, using np.where\n",
        "\n",
        "        # Embarked locations are: C = Cherbourg, Q = Queenstown, S = Southampton\n",
        "        # Embarked_C = 1, if embarked from Cherbourg, 0 otherwise\n",
        "        # Embarked_S = 1, if embarked from Southampton, 0 otherwise\n",
        "        # Embarked_ = 0 and Embarked_S = 0, if embarked from Queenstown (now Cobh ...)\n",
        "        self.dataframe['Embarked_C'] = np.where(self.dataframe['Embarked'] == 'C', 1, 0)\n",
        "        self.dataframe['Embarked_S'] = np.where(self.dataframe['Embarked'] == 'S', 1, 0)\n",
        "\n",
        "        # Remove original Sex and Embarked columns\n",
        "        self.dataframe = self.dataframe.drop(['Sex', 'Embarked'], axis=1)\n",
        "\n",
        "        # We can achieve the same one-hot encoding for Pclass using Pandas get_dummies function, instead of the\n",
        "        # manual np.where approach above\n",
        "        self.dataframe[['Pclass_1', 'Pclass_2']] = pd.get_dummies(self.dataframe['Pclass'], prefix='Pclass').iloc[:, :2].astype(int)\n",
        "        self.dataframe = self.dataframe.drop(['Pclass'], axis=1)\n",
        "\n",
        "\n",
        "        # Nomralisation\n",
        "        self.dataframe['Age_N'] = self.dataframe['Age']/self.dataframe['Age'].max()\n",
        "\n",
        "        # An example of a log transform\n",
        "        self.dataframe['log_Fare'] = np.log10(self.dataframe['Fare'] + 1)\n",
        "        self.dataframe = self.dataframe.drop(['Age', 'Fare'], axis=1)\n",
        "\n",
        "        self.dataframe.reset_index()\n",
        "        self.Train = Train\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataframe.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        if self.Train :\n",
        "            survived = self.dataframe['Survived']\n",
        "            survived = torch.tensor(np.array(survived)[idx], dtype=torch.float)\n",
        "\n",
        "        features = pd.DataFrame(columns=('Male',  'Embarked_C', 'Embarked_S', 'Pclass_1', 'Pclass_2', 'Age_N', 'log_Fare'))\n",
        "\n",
        "        # Bear in mind that the test dataset will not have a Survived column\n",
        "        if self.Train:\n",
        "            features = self.dataframe.iloc[idx,1:]\n",
        "        else:\n",
        "            features = self.dataframe.iloc[idx,:]\n",
        "\n",
        "        features = torch.tensor(features.values, dtype=torch.float)\n",
        "        if self.Train:\n",
        "            sample = (features, survived)\n",
        "        else:\n",
        "            sample = features\n",
        "        return sample"
      ],
      "metadata": {
        "id": "FF_5qNqKjnCQ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = TitanicDataset('titanic_train.csv')\n",
        "testing_data = TitanicDataset('titanic_test.csv', Train=False)\n",
        "\n",
        "sub_train_dataset, val_dataset = torch.utils.data.random_split(training_data, [0.8,0.2])\n",
        "train_dataloader = DataLoader(sub_train_dataset, batch_size=64)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "TGmqZKd8juc4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modularity\n",
        "\n",
        "We are going to now crate the various modules needed to build a basic feedforward neural network."
      ],
      "metadata": {
        "id": "VISkJf-qLgKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "besk3IPKkjUG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a Simple Feedforward Layer\n",
        "\n",
        "The following is a sample module (implementing PyTorch's abstract nn.Module), that implements a DNN layer, with a sinle weight matrix, and no bias vector.\n",
        "\n",
        "nn.Parameter is used in Torch to define any parameter (typically, but not exclusively) weights and biases), that should be updated by backpropogation.\n",
        "\n"
      ],
      "metadata": {
        "id": "aXWiM2t_Lh60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleFeedForwardLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # torch.randn (https://docs.pytorch.org/docs/stable/generated/torch.randn.html)\n",
        "        # creates a Tensor with random values extracted from a standard nomal\n",
        "        # distribution\n",
        "        # this will be a matrix of dimension: input_size x output_size\n",
        "        initial_weights_values = torch.randn(input_size, output_size)\n",
        "\n",
        "        # create a trainable weights matrix from these inital values\n",
        "        self.weights = nn.Parameter(initial_weights_values)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Compare this to the lecture notes - remeber that @ corresponds to Matrix\n",
        "        (or Tensor multiplication). So this completes the forward pass of this layer\n",
        "\n",
        "        Observe that we do x@self.weights, and not self.weights@x\n",
        "        The reason lies in the fact that a 1-D vector/flat Tensor (of shape (input_size))\n",
        "        in Pytorch correpsonds to a Linear Algebraic ROW VECTOR\n",
        "\n",
        "        If you are unsure of this, consult the lecture notes on Matehmatics, and the pdf\n",
        "        document provided on GitHub (https://github.com/andrew-nash/CS6421-labs-2026/blob/main/Lab02-RowVsColVectors.pdf)\n",
        "        '''\n",
        "        return x@self.weights\n",
        "        # equivalently, torch.matmul([x, self.weights])"
      ],
      "metadata": {
        "id": "6chFgRaNkiRn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying that this works (if you are working on the Jupyter server, or you own GPU-equipped machine or Colab instance, you may change 'cpu' to 'cuda'):"
      ],
      "metadata": {
        "id": "QC_zPS3sn_8Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l3h-8rEFF7Km"
      },
      "outputs": [],
      "source": [
        "tL = SampleFeedForwardLayer(3, 2)\n",
        "tL.to('cpu')\n",
        "test_xdata = torch.tensor([1,2,3], dtype=torch.float)\n",
        "tL(test_xdata)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graded Task 1\n",
        "\n",
        "Define a FeedForwardHeLayer, that\n",
        "\n",
        "1) Includes and uses a bias vector, as well as a weights matrix. These should be initialised as 0s. Hint: https://docs.pytorch.org/docs/stable/generated/torch.zeros.html\n",
        "2) Improve the weight initialisation to use He initialisation (you are **not** allowed to use Torch's nn.init, you must implement this from scratch\n",
        "\n",
        "\n",
        "#### He initialisation\n",
        "\n",
        "The Standard Normal initialization given above, is based on setting each weight as samples from $N(\\mu=0,\\sigma=1)$. He initialisation sets weights based on samples frm $N\\left(\\mu=0, \\sigma=\\sqrt{\\frac{2}{n}}\\right)$, where $n$ is the number of inputs to the layer.\n",
        "\n",
        "Hint: is there a mathematical operation you can do to samples from $N(\\mu=0,\\sigma=1)$, to transform them into samples from  $N(\\mu=0,\\sqrt{\\frac{2}{n}})$?\n"
      ],
      "metadata": {
        "id": "-15RVLAooLtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardHeLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        initial_weights_values = torch.randn(input_size, output_size) ..... modify to correpsond to He initialization\n",
        "\n",
        "        # create a trainable weights matrix from these inital values\n",
        "        self.weights = nn.Parameter(initial_weights_values)\n",
        "\n",
        "        initial_bias_values = ....\n",
        "        self.biases = nn.Paramater( ..... add a bias vector\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ...... Solve\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "eby3ARsFqbdY",
        "outputId": "dce886ef-d9a9-46d4-9bde-9231bf239078"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3377598297.py, line 6)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3377598297.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    initial_weights_values = torch.randn(input_size, output_size) ..... modify to correpsond to He initialization\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining an Activation Function"
      ],
      "metadata": {
        "id": "0qDzHzJgLmgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReLU activation\n",
        "\n",
        "Here, I give you the complete implmentation of a Relu Activation function as a Module:"
      ],
      "metadata": {
        "id": "QsWplhGRsCog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReluAct(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        # No nn.Parameters are needed, there are no trainable\n",
        "        # weights or biases associated with the activation\n",
        "        # function\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The most elegant approach is the following --\n",
        "        # Clamp the lowest possible value to be 0 with:\n",
        "        # return torch.clamp(x, min=0)\n",
        "        # A different method, which is more complex, but also more flexible is:\n",
        "\n",
        "        # https://docs.pytorch.org/docs/stable/generated/torch.where.html\n",
        "        # Wherever x>0, it will keep the value from x. Everywhere else, it\n",
        "        # will replace the value with 0s\n",
        "        return torch.where((x > 0), x, 0)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "qLFdchsZLmSL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Graded Task 2\n",
        "\n",
        "Implement the Sigmoid activation: $\\displaystyle \\frac{1}{1+\\exp\\left(-x\\right)}$\n",
        "\n",
        "Hint: https://docs.pytorch.org/docs/stable/generated/torch.exp.html"
      ],
      "metadata": {
        "id": "5iP4a1FJuiX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigmoidAct(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        # No nn.Parameters are needed, there are no trainable\n",
        "        # weights or biases associated with the activation\n",
        "        # function\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # implement a sigmoid activation\n",
        ""
      ],
      "metadata": {
        "id": "gxdDqrGvtb66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining A Loss Function\n",
        "\n",
        "We can define a loss function using nn.Module similarly to activation functions.\n",
        "\n",
        "Here, you are given a sample of the Mean Absolute Error, defined as\n",
        "\\begin{equation}\n",
        "l\\left(\\widehat{y}, y\\right)=\\frac{1}{n}\\sum_{i=1}^{n} \\left|\\widehat{y} - y\\right|\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "2Tc1-vSEMJ8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleMAELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, y):\n",
        "        return torch.mean(torch.abs(pred-y))"
      ],
      "metadata": {
        "id": "KudTR1qsMLpe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Graded Task 3"
      ],
      "metadata": {
        "id": "qd6gVgFNh2W3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Mean Squared Error loss, defined as\n",
        "\\begin{equation}\n",
        "l\\left(\\widehat{y}, y\\right)=\\frac{1}{n}\\sum_{i=1}^{n} \\left(\\widehat{y} - y\\right)^2\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "xjkIf-LyiCOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, y):\n",
        "        return ..... implement"
      ],
      "metadata": {
        "id": "vf4PkCXuh60W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting All this Together\n",
        "\n",
        "\n",
        "We are going to combine all of our custom modules, into a single overarching module that will define our model."
      ],
      "metadata": {
        "id": "DUkbkxIGjb8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModelFromSamples(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = SampleFeedForwardLayer(input_size, 10)\n",
        "        # because we are using modules to define our activation functions, we must\n",
        "        # store them within the model\n",
        "        self.act1 = ReluAct(input_size, 10)\n",
        "\n",
        "        self.layer2 = SampleFeedForwardLayer(10, 10)\n",
        "        self.act2 = ReluAct(10, 10)\n",
        "\n",
        "        self.layer3 = SampleFeedForwardLayer(10, output_size)\n",
        "        self.act3 = ReluAct(10, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.act3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VBLzpZD6kySt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model = SimpleModelFromSamples(7, 1)"
      ],
      "metadata": {
        "id": "WLLIAJJ8mI57"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model(sub_train_dataset[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD5WjIftmPz2",
        "outputId": "9c6ff420-b72b-4c09-95d8-cff4619d0338"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], grad_fn=<WhereBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graded Task 4\n",
        "\n",
        "class FeedForwardHeLayer(nn.Module):\n",
        "Implement a model including the and Sigmoid activation modules that you defined earlier in the Lab. It should have the followin exact structure:\n",
        "\n",
        "1. A FeedForwardHeLayer with 16 output neurons, followed by a ReLU activation\n",
        "2. A FeedForwardHeLayer with 32 output neurons, followed by a ReLU activation\n",
        "3. A FeedForwardHeLayer with 64 output neurons, followed by a ReLU activation\n",
        "4. A SampleFeedForwardLayer with 16 output neurons, followed by a Sigmoid activation\n",
        "5. A SampleFeedForwardLayer with 1 output neurons, followed by a Sigmoid activation"
      ],
      "metadata": {
        "id": "_QODxS4HnyIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        '''DEFINE YOUR LAYERS AND ACTIVATION FUNCTIONS'''\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''DEFINE THE FEEDFORWARD PROCESS'''"
      ],
      "metadata": {
        "id": "bk_OmKo0nHEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Backpropagation\n",
        "\n",
        "Unlike the last lab, we will apply the back-propogation manually."
      ],
      "metadata": {
        "id": "J-J1Ff4qML_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_sample_model(model_to_train, loss_fn, epochs, lr=0.01, batch_size=64):\n",
        "    N = len(sub_train_dataset)\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        train_loss = 0\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        for i in range((N - 1) // batch_size + 1):\n",
        "            # Get the start and end indices of ther batch\n",
        "            start_i = i * batch_size\n",
        "            end_i = start_i + batch_size\n",
        "\n",
        "            # Extract x and y data\n",
        "            x_train, y_train = sub_train_dataset[start_i:end_i]\n",
        "            pred = model_to_train(x_train)\n",
        "            loss = loss_func(pred, y_train)\n",
        "\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            # When we perform back-prop, we must tell PyTorch\n",
        "            # to pause trying to auto-computing gradients of\n",
        "            # our operations\n",
        "            with torch.no_grad():\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                # The most important part of this to pay attention to, is the .T\n",
        "                # this is because the shape of torch.round(pred) is (batch_size, 1) a matrix\n",
        "                # with a single column, whereas the shape of y_train is (batch_size) --\n",
        "                # a flat vector. We could use reshape to convert torch.round(pred) to hav\n",
        "                # shape (batch_size). Hoewver, T (transpose), does the same in this case\n",
        "                correct += (torch.round(pred).T==y_train).type(torch.float).sum().item()\n",
        "                for p in model_to_train.parameters():\n",
        "                    # here we manually define out weight and bias update\n",
        "                    # rather than using an optimizer\n",
        "                    p -= p.grad * lr\n",
        "                # reset the gradients to 0\n",
        "                # otherwise, in the next pass, the new gradients\n",
        "                # will be added to the previous ones\n",
        "                model_to_train.zero_grad()\n",
        "        correct /= N\n",
        "        print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mJms9puJO9nj"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model = SimpleModelFromSamples(7, 1)\n",
        "loss_func = SampleMAELoss()\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "batch_size=64\n",
        "fit_sample_model(sample_model, loss_fun, epochs, lr, batch_size)"
      ],
      "metadata": {
        "id": "9EU8sxVAqdg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graded Task 5\n",
        "\n",
        "Create a new training function, fit_your_model(), which trains the CustomModel you defined in Task 4.\n",
        "\n",
        "You should also compute and output the validation loss and accuracy on val_dataset in this function.\n",
        "\n",
        "You should also use you custom MSE loss for this trianing."
      ],
      "metadata": {
        "id": "N5DOebYQsyS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_your_model():\n",
        "  pass"
      ],
      "metadata": {
        "id": "GnP1ncbnq3Vr"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ....\n",
        "loss_func = .....\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "batch_size=64\n",
        "fit_your_model(model, loss_fun, epochs, lr, batch_size)"
      ],
      "metadata": {
        "id": "GV89WMQsti5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No Marks\n",
        "\n",
        "Suggestions for bonus experimentation to further your own learning and understanding (this is content that we will visit later in the course). No marks will be awarded for this, it is pirely or your own practice:\n",
        "\n",
        "1. Experiment with the number of layers, and numbers of nuerons in each layer\n",
        "2. Try different numbers epochs, different batch sizes and different learning rates\n",
        "2. Look at implementing other weight initialisations, such as Xavier initialisation\n",
        "3. Consider other activation functions, such as leaky relu\n",
        "4. Practice using the .to('cuda') funciton, to build and train models on GPU"
      ],
      "metadata": {
        "id": "4NJ53mfftoAj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pw5_VEppHUNd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}